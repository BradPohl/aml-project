{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATALOADER\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import mediapipe as mp\n",
    "import random\n",
    "\n",
    "# Define your class mappings\n",
    "class_map = {'punch': 0, 'kick': 1, 'downtime': 2}\n",
    "\n",
    "# Set up MediaPipe for pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Add Gaussian Noise\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0.0, std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn(tensor.size())*self.std+self.mean\n",
    "        return tensor+noise\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Create transformations for data augmentation\n",
    "augment_transformation = transforms.Compose([\n",
    "    # transforms.ToPILImage(), #conver to PIL image (this has many built in functions for augmenting)\n",
    "    # transforms.RandomHorizontalFlip(), #randomly flip image horizontally\n",
    "    # transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),  # Sharpening factor > 1 increases sharpness\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), #adjust brightness, contrast, saturation\n",
    "    # transforms.RandomResizedCrop(size=(64, 64), scale=(0.8, 1.0), ratio=(3/4, 4/3)),  # Random zoom and crop\n",
    "    # transforms.Resize((224,224)),\n",
    "    # transforms.ToTensor(),\n",
    "    # AddGaussianNoise(mean=0.0, std=0.05),\n",
    "    # transforms.Normalize((0.5), (0.5))\n",
    "\n",
    "\n",
    "    \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Simulate realistic variations\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Handle lighting differences\n",
    "    transforms.Resize((224, 224)),  # Ensure consistent size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "class PunchingBagDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_frames=16, frame_size=(64, 64), transform=None, indices=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_frames = max_frames  # Max number of frames to extract per video\n",
    "        self.frame_size = frame_size  # Size to resize each frame\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        # Collect all video files and labels based on folders\n",
    "        for class_name, class_idx in class_map.items():\n",
    "            class_folder = os.path.join(data_dir, class_name)\n",
    "            for file_name in os.listdir(class_folder):\n",
    "                if file_name.endswith('.mov'):\n",
    "                    self.data.append((os.path.join(class_folder, file_name), class_idx))\n",
    "\n",
    "\n",
    "        if indices is not None:\n",
    "            self.data = [self.data[i] for i in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.data[idx]\n",
    "        frames = []\n",
    "        pose_keypoints = []\n",
    "\n",
    "        # Load the video using OpenCV\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened() and frame_count < self.max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert frame to RGB and then to float32 with normalization\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.transform:\n",
    "                frame_tensor = self.transform(frame_rgb)\n",
    "            else:\n",
    "                frame_resized = cv2.resize(frame_rgb, self.frame_size)\n",
    "                frame_tensor = torch.tensor(frame_resized).permute(2, 0, 1).float() / 255.0  # Convert to float32 and normalize\n",
    "            frames.append(frame_tensor)\n",
    "\n",
    "            # Perform pose estimation\n",
    "            results = pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Extract keypoints (x, y coordinates)\n",
    "                keypoints = torch.tensor([(lm.x, lm.y) for lm in results.pose_landmarks.landmark], dtype=torch.float32)\n",
    "            else:\n",
    "                keypoints = torch.zeros((33, 2), dtype=torch.float32)  # Placeholder if no pose detected\n",
    "            pose_keypoints.append(keypoints)\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # Pad frames and keypoints if less than max_frames\n",
    "        if len(frames) < self.max_frames:\n",
    "            frames += [torch.zeros((3, *self.frame_size), dtype=torch.float32)] * (self.max_frames - len(frames))\n",
    "            pose_keypoints += [torch.zeros((33, 2), dtype=torch.float32)] * (self.max_frames - len(pose_keypoints))\n",
    "\n",
    "        # Stack frames and keypoints into tensors\n",
    "        frames = torch.stack(frames)  # Shape: (max_frames, 3, H, W)\n",
    "        pose_keypoints = torch.stack(pose_keypoints)  # Shape: (max_frames, 33, 2)\n",
    "\n",
    "        return frames, pose_keypoints, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bpohl\\anaconda3\\envs\\aml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ResNetPunchingBag(nn.Module):\n",
    "    def __init__(self, num_classes = 3):\n",
    "        super(ResNetPunchingBag, self).__init__()\n",
    "        # self.resnet = models.resnet18(weights=True)\n",
    "        # self.resnet.fc = nn.Identity() #remove's classification layer so we can classify our data\n",
    "        # self.temporal_pool = nn.AdaptiveAvgPool1d(1) #add some pooling\n",
    "        # self.fc = nn.Linear(512, num_classes)\n",
    "        self.resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "        #Freeze layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        #remove classification layer\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     batch_size, seq_len, c, h, w = x.size()\n",
    "    #     x = x.view(batch_size * seq_len, c, h, w) #Flatten temporal dimension\n",
    "    #     features = self.resnet(x) #use resnet to extract features\n",
    "    #     features = features.view(batch_size, seq_len, -1) #reshape to (B, T, F)\n",
    "    #     features = features.permute(0, 2, 1)\n",
    "    #     pooled_features = self.temporal_pool(features).squeeze(-1) # reshaped to (B,F)\n",
    "    #     # out = self.fc(pooled_features) # get output classifications\n",
    "    #     return pooled_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTModel\n",
    "\n",
    "vit = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    num_labels = 3\n",
    ")\n",
    "\n",
    "def forward_vit(vit_model, x):\n",
    "    batch_size, seq_len, c, h, w = x.size()\n",
    "    x = x.view(batch_size * seq_len, c, h, w)\n",
    "    outputs = vit_model(pixel_values=x)\n",
    "    # outputs = outputs.logits.view(batch_size, seq_len, -1) #reshape output to (B,T,C)\n",
    "    # return outputs.mean(dim=1)\n",
    "    return outputs.logits\n",
    "\n",
    "\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224-in21k'):\n",
    "        super(ViTFeatureExtractor, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name) #pretrained Vision Transformer\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(pixel_values = x)\n",
    "        return outputs.last_hidden_state[:, 0, :] #return [batch_size, hidden size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Fusion(nn.Module):\n",
    "#     def __init__(self, resnet_model, vit, num_classes=3):\n",
    "#         super(Fusion, self).__init__()\n",
    "#         self.resnet_model = resnet_model\n",
    "#         self.vit = vit\n",
    "\n",
    "#         resnet_out_dim = 512#resnet_model.fc.in_features\n",
    "#         vit_out_dim = vit.num_labels\n",
    "\n",
    "\n",
    "#         self.fc = nn.Linear(resnet_out_dim + vit_out_dim, num_classes)\n",
    "\n",
    "#     def forward(self, frames, keypoints):\n",
    "#         resnet_out = self.resnet_model(frames) #resnet output\n",
    "#         # print(f\"ResNet output shape: {resnet_out.shape}\")\n",
    "#         vit_out = forward_vit(self.vit, frames) # ViT output\n",
    "#         # print(f\"ViT output shape: {vit_out.shape}\")\n",
    "#         combined = torch.cat((resnet_out, vit_out), dim=1) #concat features\n",
    "#         # print(f\"Combined shape: {combined.shape}\")\n",
    "#         out = self.fc(combined) #final classification\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class FusionModel(nn.Module):\n",
    "#     def __init__(self, resnet_model, vit_model, num_classes=3):\n",
    "#         super(FusionModel, self).__init__()\n",
    "#         self.resnet_model = resnet_model\n",
    "#         self.vit_model = vit_model\n",
    "\n",
    "#         # Define output dimensions\n",
    "#         self.resnet_out_dim = 512  # ResNet18 output feature size\n",
    "#         self.vit_out_dim = vit_model.config.hidden_size  # ViT hidden size\n",
    "        \n",
    "#         # Fully connected classification head\n",
    "#         # self.fc = nn.Linear(self.resnet_out_dim + self.vit_out_dim, num_classes)\n",
    "#         self.fc = nn.Linear(512 + 768, 3)  # Update based on resnet_out_dim + vit_out_dim\n",
    "\n",
    "\n",
    "#     def forward(self, frames, keypoints=None):\n",
    "#         batch_size, seq_len, c, h, w = frames.size()\n",
    "\n",
    "#         # Flatten temporal dimension for ResNet\n",
    "#         frames_flat = frames.view(batch_size * seq_len, c, h, w)  # Shape: [batch_size * seq_len, c, h, w]\n",
    "\n",
    "#         # Extract features using ResNet\n",
    "#         resnet_features = self.resnet_model(frames_flat)  # Shape: [batch_size * seq_len, 512]\n",
    "#         resnet_features = resnet_features.view(batch_size, seq_len, -1)  # [batch_size, seq_len, 512]\n",
    "#         resnet_features = resnet_features.mean(dim=1)  # Average temporal: [batch_size, 512]\n",
    "\n",
    "#         # Extract features using ViT\n",
    "#         vit_features = self.vit_model(pixel_values=frames_flat).logits  # [batch_size * seq_len, 768]\n",
    "#         vit_features = vit_features.view(batch_size, seq_len, -1)  # [batch_size, seq_len, 768]\n",
    "#         vit_features = vit_features.mean(dim=1)  # Average temporal: [batch_size, 768]\n",
    "\n",
    "#         # Debugging output shapes\n",
    "#         print(f\"ResNet features shape: {resnet_features.shape}\")  # Should be [batch_size, 512]\n",
    "#         print(f\"ViT features shape: {vit_features.shape}\")        # Should be [batch_size, 768]\n",
    "\n",
    "#         # Concatenate ResNet and ViT features\n",
    "#         combined_features = torch.cat((resnet_features, vit_features), dim=1)  # [batch_size, 512 + 768]\n",
    "\n",
    "#         # Pass through classification head\n",
    "#         out = self.fc(combined_features)\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, resnet_model, vit_model, num_classes=3):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.resnet_model = resnet_model\n",
    "        self.vit_model = vit_model\n",
    "\n",
    "        # Define output dimensions\n",
    "        self.resnet_out_dim = 512  # ResNet18 output feature size\n",
    "        self.vit_out_dim = 768  # ViT hidden size (updated for ViTModel)\n",
    "\n",
    "        # Fully connected classification head\n",
    "        self.fc = nn.Linear(self.resnet_out_dim + self.vit_out_dim, num_classes)\n",
    "\n",
    "    def forward(self, frames, keypoints=None):\n",
    "        batch_size, seq_len, c, h, w = frames.size()\n",
    "\n",
    "        # Flatten temporal dimension for ResNet\n",
    "        frames_flat = frames.view(batch_size * seq_len, c, h, w)  # Shape: [batch_size * seq_len, c, h, w]\n",
    "\n",
    "        # Extract features using ResNet\n",
    "        resnet_features = self.resnet_model(frames_flat)  # Shape: [batch_size * seq_len, 512]\n",
    "        resnet_features = resnet_features.view(batch_size, seq_len, -1)  # [batch_size, seq_len, 512]\n",
    "        resnet_features = resnet_features.mean(dim=1)  # Temporal pooling: [batch_size, 512]\n",
    "\n",
    "        # Extract features using ViT\n",
    "        vit_features = self.vit_model(frames_flat)  # Shape: [batch_size * seq_len, 768]\n",
    "        vit_features = vit_features.view(batch_size, seq_len, -1)  # [batch_size, seq_len, 768]\n",
    "        vit_features = vit_features.mean(dim=1)  # Temporal pooling: [batch_size, 768]\n",
    "\n",
    "        # Debugging output shapes\n",
    "        # print(f\"ResNet features shape: {resnet_features.shape}\")  # Should be [batch_size, 512]\n",
    "        # print(f\"ViT features shape: {vit_features.shape}\")        # Should be [batch_size, 768]\n",
    "\n",
    "        # Concatenate ResNet and ViT features\n",
    "        combined_features = torch.cat((resnet_features, vit_features), dim=1)  # [batch_size, 512 + 768]\n",
    "\n",
    "        # Pass through classification head\n",
    "        out = self.fc(combined_features)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CALL AND TRAIN MODEL\n",
    "\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Initialize model, loss, and optimizer\n",
    "# model = VideoClassifier()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# def train(model, train_loader, criterion, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     for frames, keypoints, labels in train_loader:\n",
    "#         frames, keypoints, labels = frames.to(device), keypoints.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(frames, keypoints)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "# # Testing loop\n",
    "# def test(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for frames, keypoints, labels in test_loader:\n",
    "#             frames, keypoints, labels = frames.to(device), keypoints.to(device), labels.to(device)\n",
    "#             outputs = model(frames, keypoints)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#     return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# # Example of using DataLoader and training/testing loops\n",
    "# data_directory = './dataset'\n",
    "# dataset = PunchingBagDataset(data_directory)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "#     test_acc = test(model, test_loader, device)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = PunchingBagDataset(data_dir='./dataset', max_frames=16, frame_size=(224,224))\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# #Model and transformer\n",
    "# resnet_model = ResNetPunchingBag(num_classes=3)\n",
    "# vit = vit\n",
    "# fusion = Fusion(resnet_model, vit)\n",
    "\n",
    "\n",
    "# # Optimizer and Loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(fusion.parameters(), lr=1e-4)\n",
    "\n",
    "# #training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# fusion.to(device)\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     fusion.train()\n",
    "#     running_loss = 0\n",
    "#     correct_predictions = 0\n",
    "#     total_predictions = 0\n",
    "\n",
    "#     for frames, keypoints, labels in dataloader:\n",
    "#         frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = fusion(frames, keypoints)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         # Compute accuracy\n",
    "#         _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "#         correct_predictions += (predicted == labels).sum().item()\n",
    "#         total_predictions += labels.size(0)\n",
    "\n",
    "#     epoch_loss = running_loss / len(dataloader)\n",
    "#     epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Evaluate the model on the test or validation set\n",
    "# fusion.eval()\n",
    "# all_labels = []\n",
    "# all_preds = []\n",
    "# all_probs = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for frames, keypoints, labels in dataloader:  # Replace with test_dataloader if available\n",
    "#         frames, labels = frames.to(device), labels.to(device)\n",
    "#         outputs = fusion(frames, keypoints)  # Get model predictions\n",
    "#         probs = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "#         _, preds = torch.max(outputs, 1)  # Get predicted class\n",
    "        \n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "#         all_preds.extend(preds.cpu().numpy())\n",
    "#         all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# # Convert to numpy arrays\n",
    "# all_labels = np.array(all_labels)\n",
    "# all_preds = np.array(all_preds)\n",
    "# all_probs = np.array(all_probs)\n",
    "\n",
    "# # Compute metrics\n",
    "# precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "# recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "# f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# # Classification report\n",
    "# class_names = ['punch', 'kick', 'downtime']  # Update with your class names\n",
    "# print(\"\\nClassification Report:\\n\")\n",
    "# print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # AUC-ROC curves\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# # Binarize labels for multi-class AUC\n",
    "# all_labels_bin = label_binarize(all_labels, classes=range(len(class_names)))\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for i in range(len(class_names)):\n",
    "#     fpr, tpr, _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('AUC-ROC Curves')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bpohl\\anaconda3\\envs\\aml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.0699, Train Accuracy: 42.68%\n",
      "Validation Loss: 1.1730, Validation Accuracy: 35.29%\n",
      "Epoch 2, Train Loss: 0.9648, Train Accuracy: 53.50%\n",
      "Validation Loss: 1.1334, Validation Accuracy: 41.18%\n",
      "Epoch 3, Train Loss: 0.8731, Train Accuracy: 68.79%\n",
      "Validation Loss: 1.1207, Validation Accuracy: 41.18%\n",
      "Epoch 4, Train Loss: 0.7995, Train Accuracy: 80.25%\n",
      "Validation Loss: 1.0792, Validation Accuracy: 47.06%\n",
      "Epoch 5, Train Loss: 0.7179, Train Accuracy: 85.35%\n",
      "Validation Loss: 1.0547, Validation Accuracy: 47.06%\n",
      "Epoch 6, Train Loss: 0.6425, Train Accuracy: 87.26%\n",
      "Validation Loss: 1.0486, Validation Accuracy: 47.06%\n",
      "Epoch 7, Train Loss: 0.5665, Train Accuracy: 91.08%\n",
      "Validation Loss: 1.0088, Validation Accuracy: 47.06%\n",
      "Epoch 8, Train Loss: 0.4988, Train Accuracy: 93.63%\n",
      "Validation Loss: 0.9935, Validation Accuracy: 47.06%\n",
      "Epoch 9, Train Loss: 0.4335, Train Accuracy: 93.63%\n",
      "Validation Loss: 1.0206, Validation Accuracy: 47.06%\n",
      "Epoch 10, Train Loss: 0.3896, Train Accuracy: 97.45%\n",
      "Validation Loss: 1.0384, Validation Accuracy: 44.12%\n",
      "Epoch 11, Train Loss: 0.3409, Train Accuracy: 95.54%\n",
      "Validation Loss: 0.9308, Validation Accuracy: 61.76%\n",
      "Epoch 12, Train Loss: 0.2978, Train Accuracy: 99.36%\n",
      "Validation Loss: 0.9302, Validation Accuracy: 58.82%\n",
      "Epoch 13, Train Loss: 0.2543, Train Accuracy: 98.73%\n",
      "Validation Loss: 0.9397, Validation Accuracy: 58.82%\n",
      "Epoch 14, Train Loss: 0.2238, Train Accuracy: 99.36%\n",
      "Validation Loss: 0.9627, Validation Accuracy: 58.82%\n",
      "Epoch 15, Train Loss: 0.2008, Train Accuracy: 99.36%\n",
      "Validation Loss: 0.9451, Validation Accuracy: 61.76%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = PunchingBagDataset(data_dir='./dataset', max_frames=16, frame_size=(224, 224))\n",
    "\n",
    "# Get indices for splitting\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "# Split indices into training (70%), validation (15%), and test (15%)\n",
    "train_indices, test_val_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "val_indices, test_indices = train_test_split(test_val_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets for each split\n",
    "train_dataset = PunchingBagDataset(data_dir='./dataset', max_frames=16, frame_size=(224, 224), indices=train_indices, transform=augment_transformation)\n",
    "val_dataset = PunchingBagDataset(data_dir='./dataset', max_frames=16, frame_size=(224, 224), indices=val_indices)\n",
    "test_dataset = PunchingBagDataset(data_dir='./dataset', max_frames=16, frame_size=(224, 224), indices=test_indices)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "#Model and transformer\n",
    "# resnet_model = ResNetPunchingBag(num_classes=3)\n",
    "# vit = vit\n",
    "# fusion = Fusion(resnet_model, vit)\n",
    "\n",
    "# resnet_feature_extractor = ResNetPunchingBag()\n",
    "# vit_model = ViTForImageClassification.from_pretrained(\n",
    "#     'google/vit-base-patch16-224-in21k', num_labels=3\n",
    "# )\n",
    "\n",
    "# # Initialize the fusion model\n",
    "# fusion = FusionModel(resnet_model=resnet_feature_extractor, vit_model=vit_model, num_classes=3)\n",
    "\n",
    "# Initialize ResNet and ViT feature extractors\n",
    "resnet_model = ResNetPunchingBag()\n",
    "vit_model = ViTFeatureExtractor(model_name='google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Initialize the Fusion Model\n",
    "fusion = FusionModel(resnet_model=resnet_model, vit_model=vit_model, num_classes=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer and Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion.parameters(), lr=1e-5)\n",
    "\n",
    "#training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "fusion.to(device)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    # Training Phase\n",
    "    fusion.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for frames, keypoints, labels in train_loader:\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fusion(frames, keypoints)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    # Validation Phase\n",
    "    fusion.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, keypoints, labels in val_loader:\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = fusion(frames, keypoints)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bpohl\\anaconda3\\envs\\aml\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.88%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       punch       0.44      0.36      0.40        11\n",
      "        kick       0.44      0.36      0.40        11\n",
      "    downtime       0.69      0.92      0.79        12\n",
      "\n",
      "    accuracy                           0.56        34\n",
      "   macro avg       0.53      0.55      0.53        34\n",
      "weighted avg       0.53      0.56      0.54        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test Phase\n",
    "fusion.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frames, keypoints, labels in test_loader:\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        outputs = fusion(frames, keypoints)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(class_map.values()))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_map.keys(), yticklabels=class_map.keys(), cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred, target_names=class_map.keys()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
